#!/usr/bin/env python3
"""
å¹£å®‰ç¹ä¸­å…¬å‘Šç›£æ§Bot - åŠ å¼·debugç‰ˆ
å°ˆé–€ç›£æ§å¹£å®‰å…¬å‘Šä¸¦è¼¸å‡ºè©³ç´°debugè³‡è¨Š
"""

import requests
from bs4 import BeautifulSoup
import time
import json
import os
import argparse
import logging
from datetime import datetime
from config import Config
import re

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedBinanceMonitor:
    """åŠ å¼·ç‰ˆå¹£å®‰ç›£æ§Bot"""
    
    def __init__(self):
        """åˆå§‹åŒ–Bot"""
        try:
            Config.validate()
            self.telegram_token = Config.TELEGRAM_BOT_TOKEN
            self.chat_id = Config.TELEGRAM_CHAT_ID
            self.check_interval = Config.CHECK_INTERVAL
            
            # å¤šå€‹å¯èƒ½çš„ç¶²å€
            self.target_urls = [
                'https://www.binance.com/zh-TC/support/announcement/list/48',
                'https://www.binance.com/zh-TC/support/announcement',
                'https://www.binance.com/zh-TC/support/announcement/new-listing'
            ]
            
            # é—œéµå­—
            self.keywords = [
                'ä¸Šç·š', 'æ–°ä¸Šç·š', 'å³å°‡ä¸Šç·š', 'é–‹å§‹äº¤æ˜“', 
                'æ–°å¢', 'æ”¯æŒ', 'é–‹æ”¾', 'æ¨å‡º', 'å•Ÿå‹•',
                'listing', 'new trading', 'support', 'launch'
            ]
            
            self.seen_posts_file = "seen_announcements.json"
            self.seen_posts = self.load_seen_posts()
            
            # æ›´å¼·çš„è«‹æ±‚æ¨™é ­
            self.headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            logger.info("ğŸ¤– åŠ å¼·ç‰ˆå¹£å®‰ç›£æ§Botåˆå§‹åŒ–å®Œæˆ")
            logger.info(f"ğŸ¯ ç›®æ¨™ç¶²å€: {len(self.target_urls)} å€‹")
            logger.info(f"ğŸ” ç›£æ§é—œéµå­—: {', '.join(self.keywords)}")
            
        except Exception as e:
            logger.error(f"âŒ Botåˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def load_seen_posts(self):
        """è¼‰å…¥å·²è™•ç†çš„å…¬å‘Šè¨˜éŒ„"""
        try:
            if os.path.exists(self.seen_posts_file):
                with open(self.seen_posts_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data)
            return set()
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
            return set()
    
    def save_seen_posts(self):
        """å„²å­˜å·²è™•ç†çš„å…¬å‘Šè¨˜éŒ„"""
        try:
            with open(self.seen_posts_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.seen_posts), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"âŒ å„²å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
    
    def send_telegram_message(self, message):
        """ç™¼é€Telegramé€šçŸ¥"""
        url = f"https://api.telegram.org/bot{self.telegram_token}/sendMessage"
        payload = {
            'chat_id': self.chat_id,
            'text': message,
            'parse_mode': 'HTML',
            'disable_web_page_preview': False
        }
        
        try:
            response = requests.post(url, data=payload, timeout=10)
            if response.status_code == 200:
                logger.info("âœ… Telegramé€šçŸ¥ç™¼é€æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ Telegramç™¼é€å¤±æ•—: {response.text}")
                return False
        except Exception as e:
            logger.error(f"âŒ Telegramç™¼é€éŒ¯èª¤: {e}")
            return False
    
    def contains_target_keywords(self, text):
        """æª¢æŸ¥æ–‡å­—æ˜¯å¦åŒ…å«ç›®æ¨™é—œéµå­—"""
        if not text:
            return False, []
        
        text_lower = text.lower()
        found_keywords = []
        
        for keyword in self.keywords:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)
        
        return len(found_keywords) > 0, found_keywords
    
    def save_html_debug(self, content, url_index):
        """å„²å­˜HTMLå…§å®¹ç”¨æ–¼debug"""
        try:
            filename = f"debug_page_{url_index}_{datetime.now().strftime('%H%M%S')}.html"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"ğŸ“„ å·²å„²å­˜debug HTML: {filename}")
        except Exception as e:
            logger.error(f"âŒ å„²å­˜HTML debugå¤±æ•—: {e}")
    
    def extract_all_text_content(self, soup):
        """æå–æ‰€æœ‰å¯èƒ½çš„æ–‡å­—å…§å®¹ç”¨æ–¼debug"""
        all_texts = []
        
        # æå–æ‰€æœ‰æ–‡å­—å…§å®¹
        for text in soup.stripped_strings:
            if len(text) > 3:  # éæ¿¾å¤ªçŸ­çš„æ–‡å­—
                all_texts.append(text)
        
        return all_texts
    
    def fetch_and_analyze_url(self, url, url_index):
        """æŠ“å–ä¸¦åˆ†æå–®å€‹URL"""
        try:
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å–URL {url_index + 1}: {url}")
            
            response = requests.get(url, headers=self.headers, timeout=25, allow_redirects=True)
            response.raise_for_status()
            
            logger.info(f"ğŸ“¡ HTTPç‹€æ…‹ç¢¼: {response.status_code}")
            logger.info(f"ğŸ“„ å›æ‡‰å¤§å°: {len(response.content)} bytes")
            logger.info(f"ğŸ“‹ Content-Type: {response.headers.get('content-type', 'unknown')}")
            
            # æª¢æŸ¥æ˜¯å¦è¢«é‡å®šå‘
            if response.url != url:
                logger.info(f"ğŸ”„ é é¢é‡å®šå‘åˆ°: {response.url}")
            
            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')
            page_title = soup.title.string if soup.title else 'ç„¡æ¨™é¡Œ'
            logger.info(f"ğŸ“‹ é é¢æ¨™é¡Œ: {page_title}")
            
            # å„²å­˜HTMLç”¨æ–¼debug (åªåœ¨æ¸¬è©¦æ¨¡å¼)
            if url_index == 0:  # åªå„²å­˜ç¬¬ä¸€å€‹URL
                self.save_html_debug(response.text, url_index)
            
            # æå–æ‰€æœ‰æ–‡å­—å…§å®¹
            all_texts = self.extract_all_text_content(soup)
            logger.info(f"ğŸ“ æå–åˆ° {len(all_texts)} æ®µæ–‡å­—å…§å®¹")
            
            # é¡¯ç¤ºå‰10å€‹æ–‡å­—ç‰‡æ®µç”¨æ–¼debug
            logger.info("ğŸ“– å‰10å€‹æ–‡å­—ç‰‡æ®µ:")
            for i, text in enumerate(all_texts[:10]):
                logger.info(f"   {i+1}. {text[:80]}{'...' if len(text) > 80 else ''}")
            
            # å°‹æ‰¾åŒ…å«é—œéµå­—çš„å…§å®¹
            keyword_matches = []
            for text in all_texts:
                has_keyword, found_keywords = self.contains_target_keywords(text)
                if has_keyword:
                    keyword_matches.append({
                        'text': text,
                        'keywords': found_keywords
                    })
                    logger.info(f"ğŸ¯ æ‰¾åˆ°é—œéµå­—åŒ¹é…: {text[:100]}...")
                    logger.info(f"   åŒ¹é…é—œéµå­—: {', '.join(found_keywords)}")
            
            # ç‰¹åˆ¥å°‹æ‰¾éˆæ¥å’Œæ¨™é¡Œ
            links = soup.find_all('a', href=True)
            titles = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            
            logger.info(f"ğŸ”— æ‰¾åˆ° {len(links)} å€‹éˆæ¥")
            logger.info(f"ğŸ“° æ‰¾åˆ° {len(titles)} å€‹æ¨™é¡Œå…ƒç´ ")
            
            # æª¢æŸ¥æ¨™é¡Œä¸­çš„é—œéµå­—
            for title in titles:
                title_text = title.get_text(strip=True)
                if title_text:
                    has_keyword, found_keywords = self.contains_target_keywords(title_text)
                    if has_keyword:
                        logger.info(f"ğŸ¯ æ¨™é¡ŒåŒ…å«é—œéµå­—: {title_text}")
                        keyword_matches.append({
                            'text': title_text,
                            'keywords': found_keywords,
                            'type': 'title'
                        })
            
            return keyword_matches
            
        except requests.RequestException as e:
            logger.error(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•— (URL {url_index + 1}): {e}")
            return []
        except Exception as e:
            logger.error(f"âŒ åˆ†æURLæ™‚ç™¼ç”ŸéŒ¯èª¤ (URL {url_index + 1}): {e}")
            return []
    
    def run_once(self):
        """åŸ·è¡Œä¸€æ¬¡æª¢æŸ¥"""
        logger.info(f"ğŸ” é–‹å§‹æª¢æŸ¥ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        all_matches = []
        
        # æª¢æŸ¥æ¯å€‹URL
        for i, url in enumerate(self.target_urls):
            matches = self.fetch_and_analyze_url(url, i)
            all_matches.extend(matches)
            
            # å¦‚æœæ‰¾åˆ°åŒ¹é…å°±åœæ­¢æª¢æŸ¥å…¶ä»–URL
            if matches:
                logger.info(f"âœ… åœ¨URL {i + 1}æ‰¾åˆ°åŒ¹é…ï¼Œåœæ­¢æª¢æŸ¥å…¶ä»–URL")
                break
            
            # é¿å…å¤ªå¿«çš„è«‹æ±‚
            if i < len(self.target_urls) - 1:
                time.sleep(2)
        
        # è™•ç†æ‰¾åˆ°çš„åŒ¹é…
        new_alerts = []
        for match in all_matches:
            text = match['text']
            keywords = match['keywords']
            
            # ç”Ÿæˆå”¯ä¸€ID
            match_id = f"enhanced_{hash(text)}"
            
            if match_id not in self.seen_posts:
                new_alerts.append({
                    'id': match_id,
                    'text': text,
                    'keywords': keywords,
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })
                self.seen_posts.add(match_id)
        
        if new_alerts:
            logger.info(f"ğŸ¯ ç™¼ç¾ {len(new_alerts)} å€‹æ–°çš„é—œéµå­—åŒ¹é…")
            
            for alert in new_alerts:
                message = f"""
ğŸš¨ <b>å¹£å®‰é—œéµå­—è­¦å ±ï¼</b>

ğŸ“‹ <b>å…§å®¹:</b> {alert['text'][:200]}{'...' if len(alert['text']) > 200 else ''}

ğŸ¯ <b>åŒ¹é…é—œéµå­—:</b> {', '.join(alert['keywords'])}

â° <b>ç™¼ç¾æ™‚é–“:</b> {alert['timestamp']}
                """.strip()
                
                if self.send_telegram_message(message):
                    time.sleep(2)
                else:
                    logger.error(f"ç™¼é€é€šçŸ¥å¤±æ•—: {alert['text'][:50]}...")
            
            self.save_seen_posts()
        else:
            logger.info("âœ… æ²’æœ‰ç™¼ç¾æ–°çš„é—œéµå­—åŒ¹é…å…§å®¹")
    
    def test_telegram_connection(self):
        """æ¸¬è©¦Telegramé€£æ¥"""
        test_message = f"""
ğŸ§ª <b>åŠ å¼·ç‰ˆå¹£å®‰ç›£æ§Botæ¸¬è©¦</b>

âœ… Boté‹è¡Œæ­£å¸¸
ğŸ“… æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ğŸ¯ ç›£æ§ {len(self.target_urls)} å€‹ç¶²å€
ğŸ” é—œéµå­—: {', '.join(self.keywords[:5])}ç­‰

ğŸš€ æº–å‚™é–‹å§‹è©³ç´°ç›£æ§ï¼
        """.strip()
        
        logger.info("ğŸ” æ¸¬è©¦Telegramé€£æ¥...")
        return self.send_telegram_message(test_message)
    
    def run_daemon(self):
        """æŒçºŒç›£æ§æ¨¡å¼"""
        logger.info("ğŸ¤– åŠ å¼·ç‰ˆå¹£å®‰ç›£æ§Boté–‹å§‹é‹è¡Œï¼")
        
        start_message = f"""
ğŸ¤– <b>åŠ å¼·ç‰ˆç›£æ§Botå·²å•Ÿå‹•</b>

â° æª¢æŸ¥é–“éš”: {self.check_interval}ç§’
ğŸ¯ ç›£æ§å¤šå€‹å¹£å®‰å…¬å‘Šç¶²å€
ğŸ” é—œéµå­—: {', '.join(self.keywords)}
ğŸ“„ è©³ç´°debugæ¨¡å¼å·²é–‹å•Ÿ

é–‹å§‹ç›£æ§...
        """.strip()
        
        self.send_telegram_message(start_message)
        
        try:
            while True:
                self.run_once()
                time.sleep(self.check_interval)
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ Botå·²åœæ­¢")
            self.send_telegram_message("ğŸ›‘ <b>åŠ å¼·ç‰ˆç›£æ§Botå·²åœæ­¢</b>")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='åŠ å¼·ç‰ˆå¹£å®‰ç›£æ§Bot')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼')
    parser.add_argument('--test-bot', action='store_true', help='æ¸¬è©¦é€£æ¥')
    
    args = parser.parse_args()
    
    try:
        bot = EnhancedBinanceMonitor()
        
        if args.test_bot:
            success = bot.test_telegram_connection()
            print("âœ… æ¸¬è©¦æˆåŠŸ" if success else "âŒ æ¸¬è©¦å¤±æ•—")
        elif args.test:
            bot.run_once()
        else:
            bot.run_daemon()
            
    except Exception as e:
        logger.error(f"âŒ ç¨‹å¼éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()